[{"content":"Ready reference for MLS-C01 and SAP-C01: Amazon Kinesis Streams Compared  No need to study and find out inputs and outputs supported by Kinesis Streams by digging into AWS Documentation Here is a ready to refer comparision of Kinesis streams available on AWS Score points in exam:  Know the right sources and sinks that each stream supports Be cautious about tricky exam questions that state a wrong answer option involving kinesis streams    Kinesis Video Streams - KVS  Though the name is Kinesis Video Streams, it supports ingestion of any kind of time series or time encoded data Video:  Stream video frames to KVS for storage, analytics, machine learning, playback or custom video processing Kinesis Video Streams provides fully-managed capabilities for HTTP Live Streaming (HLS) and Dynamic Adaptive Streaming over HTTP (DASH). Kinesis Video Streams also supports ultra-low latency two-way media streaming with WebRTC, as a fully managed capability   Time Encoded Data:  Definition: Time-encoded data is any data in which the records are in a time series, and each record is related to its previous and next records e.g. video Non video examples, scientific applications:  LIDAR: LIght Detection And Ranging, is a remote sensing method that uses light in the form of a pulsed laser to measure ranges (variable distances) to the Earth RADAR: RAdio Detection And Ranging, is a detection system that uses radio waves to determine the range, angle, or velocity of objects. It can be used to detect aircraft, ships, spacecraft etc. Audio: any audio data     Write using:  PutMedia API Producer SDK   Read using:  GetMedia API Kinesis Video Streams Parser Library which can parse output from GetMedia API      Kinesis Video Streams I/O Integrations   Kinesis Data Streams - KDS  Write clickstreams, application logs, social media and thousands of sources to Amazon Kinesis data stream Write data to KDS using:  Kinesis Producer Library that manages concurrency and generates high throughput within limits of Kinesis Data Streams Use Kinesis API like PutRecords() from any compute e.g. Lambda, EC2 etc. and write to KDS Using kinesis agent   Read using:  Kinesis Client Library, from any app GetRecords() API Subscribe a Lambda where KDS is event source for a lambda      Kinesis Data Streams I/O Integrations   Kinesis Data Firehose - KDF  KDF is mainly meant only for data delivery unlike other streams It delivers realtime data from a source to various destinations with a transformation applied if need be Built-in support for JSON data to be written to Apache Parquet format to S3, no need of coding  Any other format, other than JSON, needs coding e.g. CSV These are called as lambda blueprints   Refer diagram below, to know data delivery destinations supported You can add data to an Amazon Kinesis Data Firehose delivery stream through Amazon Kinesis Agent or Firehose’s PutRecord and PutRecordBatch operations Kinesis Data Firehose is also integrated with other AWS data sources such as Kinesis Data Streams, AWS IoT, Amazon CloudWatch Logs, and Amazon CloudWatch Events Can write to firehose using kinesis agent  After installing Amazon Kinesis Agent on your servers, you can configure it to monitor certain files on the disk and then continuously send new data to your delivery stream. For more information, see Writing with Agents      Kinesis Data Firehose I/O Integrations   Kinesis Data Analytics - KDA  Realtime data analytics on streaming data using SQL Apply ML algorithms and perform anomaly detection  Anomaly detection is just an inbuilt SQL function that internally uses RCF - Random Cut Forest algorithm   Use Apache Flink applications in a supported language such as Java or Scala using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale Remember: KDA Cannot directly write to S3, attach a KDF i.e firehose to deliver KDA data to S3    Kinesis Data Analytics I/O Integrations   Tabular Comparison   Compare various streams at a glance   Leave your comments to appreciate the article or request for change. Thanks!\n","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/ai-ml-kinesis-compare/","summary":"Ready reference for MLS-C01 and SAP-C01: Amazon Kinesis Streams Compared  No need to study and find out inputs and outputs supported by Kinesis Streams by digging into AWS Documentation Here is a ready to refer comparision of Kinesis streams available on AWS Score points in exam:  Know the right sources and sinks that each stream supports Be cautious about tricky exam questions that state a wrong answer option involving kinesis streams    Kinesis Video Streams - KVS  Though the name is Kinesis Video Streams, it supports ingestion of any kind of time series or time encoded data Video:  Stream video frames to KVS for storage, analytics, machine learning, playback or custom video processing Kinesis Video Streams provides fully-managed capabilities for HTTP Live Streaming (HLS) and Dynamic Adaptive Streaming over HTTP (DASH).","title":"Amazon Kinesis Streams Compared"},{"content":"Ready reference for MLS-C01: Sagemaker Algorithms Compared  Study following table to compare various built-in algorithms in Sagemaker You can use this as a ready reckoner for MLS-C01 AWS Certified Machine Learning Specialty Exam Pay attention: Scroll the table horizontally for more columns Download the table as PDF here   Algorithm Algo. Type Input Format INT/FLOAT Processor Type Instance Multiprocessor\nin Single Machine Multi Machine Use Cases Comments HP     Linear Learner SUPERVISED • RecordIO Wrapped Protobuf / CSV\n• Float32 Data only\n FLOAT32 CPU\nGPU Any CPU\nGPU Only CPU\nNo GPU • Regression and classification\n• Classification: Binary or multiclass • Need data to be normalized else algo may not converge\n• Multiple models are trained in parallel balance_multiclass_weights\nlearning_rate\nmini_batch_Size\nL1, L2   XGBoost SUPERVISED • CSV or LibSVM (not AWS algo, but adapted, hence NO RecordIO-protobuf) - CPU M4 - No • Regression and classification\n• Classification: Binary or multiclass • Output model as pickle\n• Uses extreme boosting of trees\n• Algo is memory bound, not much compute • subsample_trees (less overfitting)\n• eta (eq. to learning rate)\n• alpha, gamma, lambda (conservative trees for higher values)   Seq2Seq SUPERVISED • RecordIO-Protobuf INT GPU P3 GPU No • Machine translation\n• Text summarization\n• Speech to text\n• Any use case where input a sequence and output is a sequence • Along with training data and validation data files, must provide vocabulary files \u0026ndash; in case of text seq2seq\n• Start with tokenized text files, then convert to RecordIO-Protobuf\n• Uses RNNs and CNNs internally • batch_size\n• optimizer\n• learning_rate\n• num_layers_encoder\n• num_layers_decoder\n• can optimize on: accuracy, BLEU score (mach. translation), perplexity\n   DeepAR SUPERVISED • JSON Lines\n• GZIP\n• Parquet\n-- Each record to contain\n- Start: starting TS\n- Target: the TS values to learn/predict - CPU\nGPU C4\nP3 CPU\nGPU CPU\nGPU • Stock price prediction\n• Sales and promotion effectiveness\n• Any time oriented forecasting, single dimension • Uses RNNs\n• Can train several related timeseries, more series the better results, learns relationships b/w timeseries\n• Start with CPU (C4.2xlarge, or higher), if necessary, move to GPU. Only large models need GPU • context_length (number of time points back in time the model learns)\n• epochs, batch_size, learning_rate, num_cells   Blazing Text - Text Classification SUPERVISED Augmented manifest text format \u0026ndash;\n\u0026quot;__label__1 this is a sentence with , punctuations also tokenized . that is space delimited . One sentence per line . label at the start\u0026quot; - CPU\nGPU size \u0026lt; 2GB: C5\nsize \u0026gt; 2GB: P2, P3 Single GPU No • web search and information retrieval • predict labels for sentence • epochs\n• learning_rate\n• word_ngrams\n• vector_dim   Blazing Text - Word2Vec UNSUPERVISED Word2Vec\none sentence per line\n - CPU\nGPU P3 CPU/GPU: CBOW \u0026amp; Skip Gram GPU: Batch skip gram\nCPU: No • Preparing input for NLP use cases\n• Vectorization of text for machine translation and sentiment analysis\n• Semantic similarity of words • Represents words as vectors\n• Semantically similar words are represented by vectors close to each other\n• Semantic \u0026ndash; of or relating to meaning in language\nMULTIPLE MODES:\n• CBOW - Continuous Bag of Words - Order of words DO NOT matter\n• Skip Gram i.e. n-gram - order of words matter\n• Batch skip gram - order of words matter\n • mode: mandatory\n• learning_rate\n• window_size\n• vector_dim\n• negative_Samples   Object2Vec  • Any object to be tokenized into integers\n• Training data:\n- pairs of tokens\n- sequence of tokens INT CPU\nGPU M5, P2 Single machine No • Collaborative recommendation system\n• Multi-label document classification system\n• Sentence Embeddings\n• Learns relations or associations:\n- sen to sen\n- labels to seq (genre to description)\n- product to product (recommendation)\n- user to item (recommendation) • CNNs and RNNs used\n• Encoders used in input\n- uses 2 encoders in parallel\n- learns associations b/w encoders, using a comparator\nEncoder types:\n• Hierchical CNNs (hCNNs)\n• bi-lstm\n• pooled_embedding\n dropout, early_stopping_ epochs, learning_rate, batch_size, layers, act. func., optimizer, weight_decay   Object Detection SUPERVISED RecordIO (NOT Protobuf) or Images (JPEG or PNG)\n+\nWith image manifest in JSON, one JSON per image\nthat contains annotations\n - GPU P2, P3 Yes Yes • Detect objects in an image\n• Object tracking • Uses CNN with SSD\n• Transfer learning/incremental learning supported\n• Uses FLIP, RESCALE, JITTER internally to avoid overfitting\n• CPUs can be used for inference, not for training Standard CNN HPs like: learning_rate, batch size, optimizer etc.   Image Classification SUPERVISED • Pipe: Apache MxNET RecordIO (NOT Protobuf) - for interoperability with other DNN frameworks\n• File Mode: Raw JPEG/PNG + *.LST files - associates image index, class label, path to image\n-- To use images directly in Pipe mode use JSON based Augmented Manifest Format - GPU P2, P3 Yes Yes • classify images into multiple classes\n• dog/cat/rat/tiger etc. • Full training: ResNet CNN is used. N/W initialized with random weights\n• Transfer Learning/Pre-trained: Image Net is used. Initialized with pre trained weights. Only Top FC layer is initialized with random weights.\n• CPU can be used for inference, if not suitable, move to GPU • batch_size\n• learning_rate\n• optimizer, B1, B2, eps, Gamma   Semantic Segmentation SUPERVISED • Raw JPEG/PNG in file mode + annotations\n• Add Augmented Manifest Format for Pipe Mode - GPU P2, P3 Yes No • Self driving cars\n• Medical imaging and diagnostics\n• Robot sensing\n• Given a pixel - what object does it belong to ? • Algo under hood: Gluon CV of MxNET = FC + Pyramid Scene Pairing + DeepLabV3\n• Arch: ResNet50/ResNet101 = \u0026ldquo;Backbone\u0026rdquo; selection in HP\n• Trained on ImageNet data\n• Incremental/Transfer learning allowed\n• Inference can use CPU or GPU\nEach of the three algorithms has two distinct components:\n• The backbone (or encoder)—A network that produces reliable activation maps of features.\n• The decoder—A network that constructs the segmentation mask from the encoded activation maps.\nThe segmentation output is represented as a grayscale image, called a segmentation mask. A segmentation mask is a grayscale image with the same shape as the input image. epochs, learning_rate, batch size, algo, backbone   Random Cut Forest UNSUPERVISED • RecordIO-Protobuf\n• CSV - CPU M4,C4,C5 - No • Anomaly detection\n• Detect unexpected spikes in TS data\n• Few people have tried using this for fraud detection • Assigns anomaly score to each data point\n• Uses forest of trees\n• Looks at expected change in complexity as a result of adding a point to a tree\n• Random sampling\n• RCF is used in Kinesis Analytics in real time num_trees, num_samples_per_tree (= choose inversely proportional to ratio #anomalous/#normal in dataset)\n   Neural Topic Modelling UNSUPERVISED • RecordIO-Protobuf\n• CSV\n- Words must be tokenized to integers\n- aux channel for vocab INT GPU P2, P3 -  • Organize docs into topics\n• Summarize docs based on topics • Algo: Neural Variational Inference\n• Define how many topics to group docs into\n• Used only on text\n• CPU / GPU for inference num_topics\nmini_batch_size\nlearning_rate\nvariation_loss (at expense of learning time)   LDA (Latent Dirichlet Allocation) UNSUPERVISED • RecordIO-Protobuf (Pipe Mode)\n• CSV\n- Words must be tokenized to integers\n- aux channel for vocab - CPU M4 No No • Cluster customers based on purchases\n• Harmonic analysis in music • Algo: LDA - Open source availability, not DNN\n• Can process more than text, like harmonic music analysis\n• Single inst. CPU\n num_topics\nalpha0 = small values - sparse topic mixtures, \u0026gt;1 uniform topic mixture   kNN (k Nearest Neighbors) SUPERVISED • RecordIO-protobuf\n• CSV\n-- File or pipe mode both\n- first column has label - CPU\nGPU - - - • Classification and regression • Sagemaker automates 3 steps:\n- Sample data (can\u0026rsquo;t use for huge data)\n- Dim reduction (sign or nfjlt methods)\n- Build index for looking up neighbours k\nsample_size   K-Means UNSUPERVISED • RecordIO-protobuf\n• CSV\n-- File or pipe mode both - CPU (recommended)\nGPU M4, M5, C4, C5 - - • Cluster data - unsupervised\n• Find groups of data points based on similarity • Webscale K-Means in Sagemaker\n• Similarity measured by euclidean distance\n• Works to optimize the centers of eack of the k-clusters\n• Algorithm:\n1) Determine init. cluster centers = 2 ways: k-means++ (tries to make initial clusters far apart) OR random\n2) Iterate over data and calculate cluster center\n3) Reduce from K to k - using Lloyd\u0026rsquo;s method or k-means++\nK comes from \u0026ldquo;extra_cluster_centers\u0026rdquo; which improves accuracy, but later reduced to k.\nK = k • x • K\n• mini_batch_size\n• extra_center_factor (x)\n• init_method (k-means++ OR random)   PCA - Principal Component Analysis UNSUPERVISED • RecordIO-protobuf\n• CSV\n-- File or pipe mode both - CPU\nGPU - - - • Dimensionality Reduction\n• Removes Curse of Dimensionality • Reduced Dimensions are called components\n• 1st component - largest possible variaility, next 2nd component, so on ..\n• Used Singular Value Decomposition (SVD)\n• Two Modes:\n- Regular: Sparse data. modelate #features, #rows\n- Randomized: Dense data. #large data, #large features, uses approximation algos • algorithm_mode (regular, random\n• subtract_mean: unbiases data   Factorization Machines SUPERVISED • RecordIO-Protobuf FLOAT32 CPU (recommended)\nGPU - - - • Regression, Classification, recommendation - all in one general purpose algo for sparse data\n• Click prediction\n• Item recommendation • Limited to pairwise interaction - 2nd order\ne.g. user to item interactions\n• CSV not practical hence not supported,a s data is sparse\n*GPU not recommented as data is sparse, GPU works better on dense data • Initialization methods for bias, factors and linear terms\n- methods: uniform, normal or const\n- can tune properties of each method   IP Insights UNSUPERVISED • CSV only for training\n• Inference: JSON lines, CSV, JSON - CPU\nGPU (recommended) - Multi GPU - • Identify suspicious IP addresses in context of security\n• Logins from anomalous IPs\n• Identify accounts creating resources from anamolous IPs • Only IPv4 supported\n• Uses NN to learn latent vector rep. of entities and IP addresses\n• entities are hashed and embedded - large hash size\n• Automatically generates anomalous data by randomly pairing entities and IPs - as data will be highly imbalanced • num_entity_vectors (hash size, set to twice the unique entity identifiers)\n• vector_dim (size of embedding vectors, scales model size)\n• Others: epoch, batch_size, leraning_rate, etc.   Reinforcement Learning REINFORCEMENT LEARNING • Nothing specific to Sagemaker - GPU GPU Yes Yes - Multi Instance GPU recommended • Games\n• Supply chain management\n• HVAC Systems\n• Industrial robotics\n• Dialog systems\n• Autonomous vehicles • Supports Intel coach, Ray RLLib\n• Tensorflow, MxNET\n• Custom, commercial and opensource environments supported - Matlab simulink, energy plus, robo school, pybullet, Amazon Sumerian, AWS Robomaker • Depends on framework and algo used, nothing tied to Sagemaker     Algorithms categorized by feature  Here a feature is based on:  Data type of input Algorithms that can train on CPU or GPU Algorithms that can be trained incrementally  Incremental training enables to resume training, or to retrain a DNN by changing the final FC layer by using a pre-trained model i.e transfer learning       Mandatory FLOAT32 Mandatory INT32 CPU ONLY GPU Only Incremental Training Available     Linear Learner Seq2Seq XGBoost Seq2Seq Image Classification   Factorization Machines Object2Vec RCF Image Classification Semantic Segmentation    NTM LDA Semantic Segmentation Object Detection      Object Detection       NTM       Reinforcement Learning      Algorithms that support distributed training  Mnemonic to remember: F-SKILBDR as in SkillBuilder (If you can come up with something better, please share in comments! ) An entire new blog to understand distributed training and how it works coming soon!   Distributed Training Support     Factorization Machine   Seq2Seq   K-Means   IP Insights   Linear Learner (Not LDA)   Blazing Text - Word2Vec   DeepAR   RCF - Random Cut Forests     Leave your comments to appreciate the article or request for change. Thanks!\n","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/ai-ml-mls-c01/","summary":"Ready reference for MLS-C01: Sagemaker Algorithms Compared  Study following table to compare various built-in algorithms in Sagemaker You can use this as a ready reckoner for MLS-C01 AWS Certified Machine Learning Specialty Exam Pay attention: Scroll the table horizontally for more columns Download the table as PDF here   Algorithm Algo. Type Input Format INT/FLOAT Processor Type Instance Multiprocessor\nin Single Machine Multi Machine Use Cases Comments HP     Linear Learner SUPERVISED • RecordIO Wrapped Protobuf / CSV","title":"Amazon Sagemaker Built-in Algorithms Compared"}]