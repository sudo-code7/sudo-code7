[{"content":"We need to learn about bias and variance in order to understand how to fix underfitting and overfitting of model. Spend enough time to read and understand, everything till the end of the blog, where you get to know how to apply the concepts (helpful for MLS-C01 exam too!)\nWhat are Bias and Variance ? Welcome to the world of bias and variance. It may be confusing at first, but by end of the blog, you\u0026rsquo;ll have full clarity. When learning about bias and variance, you need to be very clear about the context where it\u0026rsquo;s used.\n There is a statistical way of understanding it on data, used in descriptive statistics Bias and variance in the context of ML model training, where in underfit and overfit terms are used interchangeably, respectively  To be clear, in ML model training:  Bias and underfit are used interchangeably Variance and overfit are used interchangeably      We\u0026rsquo;ll learn it in the context of Data/Statistics at first then in the context of Machine Learning\nBias and Variance in Data  Bias and variance are one of the characteristics of data. Bias is more of error introduced in data due to the nature of data collection process used, where as, variance is a statistical measure Remember this is not what ML Engineers refer to when they talk about bias-variance tradeoff, rather, it\u0026rsquo;s in context of model training using the data (More about this later)  Bias in Data Data bias in statistics is type of error in which certain elements of a dataset are more heavily weighted and/or represented than others. A biased dataset does not accurately represent the real world perception of a concept or reality under study, resulting in skewed outcomes, low accuracy levels, and errors when such a data is used in machine learning\nLet\u0026rsquo;s consider an example:\n Let\u0026rsquo;s say we want to predict income of individuals in a country, but it\u0026rsquo;s really difficult to collect such information from the whole population For our study, statistically, we can collect data in such a way that it nearly represents all the strata of society i.e both rich and poor, all races, religions etc. However some regions of a country have may have really affluent people than other regions If we don\u0026rsquo;t randomly sample data across the country we may end up in collecting skewed data that we may assume to represent the country This kind of skewed data is due to incorrect sampling or collection is called as Sample bias  There are many kinds of bias, let\u0026rsquo;s consider a few common one\u0026rsquo;s here:\n Exclusion bias: Most often it’s a case of deleting valuable data thought to be unimportant. However, it can also occur due to the systematic exclusion of certain information. For example in sales data, 99% of the sales may be from Product A, however 1% of Product B sales could be contributing to the 50% of revenue. Excluding Product B thinking it\u0026rsquo;s not contributing to sales can be a disaster. Observer bias or Confirmation bias: is the effect of seeing what you expect to see or want to see in data. This can happen when researchers go into a project with subjective thoughts about their study, either conscious or unconscious   Sudo Tip: Bias in data, gets injected in the model learnt by ML Algorithms and can result in bad predictions lowering the accuracy of the model. Identifying and removing bias in data is an important step in data preparation for ML.\n    Variance of Data  Well, statistics is the root of traditional machine learning algorithms, there is not doing away with statistics when data is in focus Variance in data, is nothing but a measure to describe how spread out the data is  E.g. Variance is trying to describe how much a set of numbers vary One way to do that is to describe a number range i.e. min is 5 and max is 500 from a given set of numbers But there are issues in using such a double number measure (min, max), instead a single number measure is used Refer this awesome guide, to refresh your memory After reading above guide, you\u0026rsquo;ll realize standard deviation is another measure of variance, and a better one     Sudo Tip: Variance in data is calculated at feature (column of data prepared for ML algorithm training) level, and variance is considered good.\n Features that have low variance - contribute very less towards learning and hence are removed from training dataset Features that have high variance, help in describing patterns in data, thereby helps an ML model to learn them      Bias and Variance in ML Model Having understood Bias and Variance in data, now we can understand what it means in Machine Learning models\n Bias and variance in a model can be easily identified by comparing the data set points and predictions Above figure shows an example for a regression case The blue dots are training data points The red line is the regression line learnt (or as it\u0026rsquo;s called fit a curve to data) by ML algorithm Overfit/High Variance:  The line fit by algorithm is so tight to the training data that is cannot generalize to new unseen data This case is also called as high variance in model because, the model has picked up variance in data and learnt it perfectly. The high variance in data could be because of noise, and when learnt by model, it lowers accuracy of model We should avoid overfit models to generalize better on new data (keep reading to know how to reduce overfit in models)   Underfit/High Bias:  The line fit by algorithm is flat i.e constant value. No matter what is the input, prediction is a constant. This is the worst form of bias in ML The algorithm has learnt so less from data that the line has been underfit (due to high bias) We should avoid underfit models (keep reading to know how to reduce underfit in models)   Good Balance/Right Fit  The right fit models learn a smooth curve on data representing the direction of data and variation in data    Bias-Variance Trade-off  There is a fancy term called bias-variance tradeoff which simply means you cannot reduce both bias and variance in model You can only achieve a good balance between A good analogy would be: one cannot achieve both high speed and torque at the same time. Higher the torque, lower the speed and vice-versa Less geeky analogies:  When you have to put up with a half hour commute in order to make more money You might take a day off work to go to a movie, gaining leisure and entertainment, while losing a day\u0026rsquo;s wages   The point of a trade-off is that you can\u0026rsquo;t have both  The closure: Important for MLS-C01 exam and for your learning How to identify high bias (underfit) and high variance (overfit) in a model ?  Sudo Exam Tip: Below graph is important to recognize bias and variance cases in training. Remember to identify the exact case by looking at gap between the training and validation curves towards the end of the curve\n     First graph: A high training error (underfit) and a small gap between the train and validation error curves, towards the end of curve indicates high bias, inturn implies low variance Second graph: A low training error (overfit) and large gap between the train and validation error curves, towards the end of curve indicates low bias, inturn implies high variance  How to fix high bias (underfit) and high variance (overfit) in models ?  Sudo Exam Tip: Remember and memorize the following, after you understand it thoroughly. Write it down in the given rough sheet of paper at exam center as soon as you enter to save time. Many questions need you to have below information at the back of your hand, meaning, committed to memory.\n    ","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/fixing-overfit-and-underfit-models/","summary":"We need to learn about bias and variance in order to understand how to fix underfitting and overfitting of model. Spend enough time to read and understand, everything till the end of the blog, where you get to know how to apply the concepts (helpful for MLS-C01 exam too!)\nWhat are Bias and Variance ? Welcome to the world of bias and variance. It may be confusing at first, but by end of the blog, you\u0026rsquo;ll have full clarity.","title":"Bias-Variance and Model Underfit-Overfit demystified!"},{"content":"Courtesy: Photo by Green Chameleon on Unsplash\nWhat is MLS-C01 and how is it helpful ? If you crack the MLS-C01 exam, I can assure you that it can help you in your role at your current company, may be a promotion or some other benefit. It definitely helped me to set myself apart from the crowd in my company, and helped me get a promotion as well. It depends on person to person and opportunities in company.\nThe certification preparation, definitely helped me get a broader perspective, and enhance my learning beyond my area of expertise ie. traditional ML algorithms for a long time.\n The best part about MLS-C01 is that it tests practical knowledge on your experience in developing and debugging models Around 40% to 50% of questions test the knowledge you have in data science, data engineering, feature engineering and model training  The exam can touch upon any algorithm outside of built-in algorithm in Sagemaker - not like any ML algorithm on earth, but the most used and popular ones Need to have a good knowledge on deep learning frameworks and and building models using them   Rest of the questions will be from AWS Sagemaker, its built in algorithms, training on AWS Sagemaker and using AWS resources to deploy models in production People who do not have experience in ML from their job have also passed the exam, but they have put in their efforts as well  How did I prepare ? Most of my learning comes from my on job experience. However, everyone needs exam specific preparation, job experience doesn\u0026rsquo;t cover the breadth of the exam\nVideo tutorials  There are many video tutorials in Udemy, I specifically used the one from Frank Kane and Stephane Mareek Another video tutorial and a good bank of practice questions are available on www.aws.training  It is the exam readiness course on aws.training which has a practice exam for free of cost, containing 35 questions (at the end of the course) At the end of every module, it has 4 to 5 questions which are again bonus preparation material Above is a game changer and helps gain confidence   Another source would be to purchase practice exams on Udemy and go through them again and again. Test yourself until you score above 95%  Few pointers from my experience on exam  tSNE technique comes up very often. Read as much as you can about tSNE as a dimensionality reduction technique and as a visualization technique Read about how to calculate TP, TN, FP, FN using a different kind of confusion matrix which has them represented as heat map and percentages. It also has F1 scores and total samples as data represented on confusion matrix. I couldn\u0026rsquo;t answer this kind of a question. Better to be prepared  A confusion matrix is very confusing. Until you take a paper and a pen, solve many numerical questions, you can never master it Go back to college mode, practice this again and again   More of machine learning concepts, how to reduce over fitting, how to reduce under fitting, both in cases of traditional ML algorithms and DNN DNN concepts, hyperparameters, architectures in DNN ( CNN, RNN etc) - check my blog on hyperparameters for more details. Kinesis streams in deep and the kind of integrations supported in detail - check my blog on kinesis streams Builtin algorithms in deep - should know what they support and don\u0026rsquo;t support. Check my built-in algorithms comparison blog Tricky questions in statistics and data preparation  About distributions being right tailed or left tailed What kind of transformations to use to make them normal   Identification of which distribution to use, when a scenario is given e.g. number of times a bus arrives at a station in a given interval would be Poisson  Similarly study all other distributions   If you don\u0026rsquo;t have experience in ML - Focus on Data science techniques, data exploration and statistics part. Else this will be easy and you\u0026rsquo;ll have to focus more on Sagemaker and MLOps (vice versa if you have sagemaker experience) MLOps using sagemaker  Deployments A/B testing Canary deployments i.e. using [production variants(https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html)] Setting up autoscaling Multi model deployments etc.   Know data stores very well (based on use case) i.e. OLTP and OLAP difference How to form data lakes. Cost of storage i.e. EFS, EBS, instance store, S3, etc. Know when to use redshift v/s athena v/s/ redshift spectrum Few security concepts on securing data at rest and in transit, VPCs with Sagemaker etc  Sudo-code Tips  As soon as you enter the exam center (I prefer exam center over online tests at home), write down all the formulae you need for the exam, on the rough sheet given to you. This serves as a quick reference and saves a lot of time  Write down a confusion matrix in two ways, this will help pick right numbers from right cells in the matrix for calculations  Predicted on x-axis and Actual on y-axis The other way round, i.e. Predicted on y-axis and Actual on x-axis   Write down formulae for the following  Recall or Sensitivity or True Positive Rate Precision or Positive Predictive Value Specificity or True Negative Rate F1 Score in two ways  Using TP, FP and FN Using Precision and Recall     Write down formula for calculating number of kinesis shards Algorithm specific formulae e.g. TF-IDF calculation    I wish you all the best for your exam! Take a leap of faith with right preparation, and you shall conquer!\n","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/mls-c01-exap-prep-pass-guide/","summary":"Courtesy: Photo by Green Chameleon on Unsplash\nWhat is MLS-C01 and how is it helpful ? If you crack the MLS-C01 exam, I can assure you that it can help you in your role at your current company, may be a promotion or some other benefit. It definitely helped me to set myself apart from the crowd in my company, and helped me get a promotion as well. It depends on person to person and opportunities in company.","title":"How to pass MLS-C01 Exam? How to Prepare?"},{"content":"What are hyperparameters ? In simple terms, hyperparameters are a set of knobs you can tune before starting the learning process in machine learning. When set to a specific values, the model has a specific performance. When the hyperparameters are set to new values, the model gets new performance values.\nWhat is performance of a model? In any machine learning process, we want to optimize the objective-function: a fancy name for a function that evaluates model predictions v/s values provided during training. A gradient descent algorithm can find a minima of an objective function. For example, in regression, the closer the predicted values are to real values, the better the model performance.\nHow changing a hyperparameter impacts model This blog aims to show an impact analysis when mini-batch size and learning rate are used as control variables. The impact of changing these controls are studied on following variables:\n Number of iterations/epochs Algorithm convergence speed Model performance and accuracy Chance of finding better or global minimum Noise in cost/evaluation metric  Knowing the impact of changing hyperparameters: mini-batch size and learning rate are very important in every day work of a Machine Learning Engineer or a Data Scientist. Also, very important for the MLS-C01 exam.\nImpact of lowering Mini Batch Size The following statements are general thumb rules and can vary depending on specific cases of model training, however they are generally true.\n Lower mini batch size leads to lower number of iterations to achieve similar model performance, meaning, larger batch size requires higher number of iterations  Remember that:  mini batches use n number of data points in an epoch SGD - stochastic gradient descent uses 1 data point in an epoch Gradient descent uses all data points in the data set in one epoch   Think of it this way, larger the batch size, a model cannot generalize well and vice-versa   Lower number of iterations, obviously would lead to faster convergence Lowering mini batch size increases noise in the objective function or cost function The increase in noise, enables the gradient to jump out of local minima, thereby increasing chances of finding a global minima Lowering mini batch size also improves model performance, when better minima is found  Impact of lowering Learning Rate The following statements are general thumb rules and can vary depending on specific cases of model training, however they are generally true.\n Lower learning rate would generate smaller gradient, which would increase the iterations Higher the iterations, slower the convergence Smaller the gradients, better the model performance as the cost would be lower Chance of finding the global minima decreases as noise is lesser i.e. won\u0026rsquo;t be able to jump out of local minima  It\u0026rsquo;s a good thing if we are traversing towards global minima, but not if in local minima path    A diagram for ready reference to see the impact chain ","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/ai-ml-hyperparameters/","summary":"What are hyperparameters ? In simple terms, hyperparameters are a set of knobs you can tune before starting the learning process in machine learning. When set to a specific values, the model has a specific performance. When the hyperparameters are set to new values, the model gets new performance values.\nWhat is performance of a model? In any machine learning process, we want to optimize the objective-function: a fancy name for a function that evaluates model predictions v/s values provided during training.","title":"How hyper-parameters impact model training?  (with MLS-C01 focus)"},{"content":"Courtesy: Wikimedia\nWhat is segmentation of image in machine learning? In the field of computer vision in machine learning, an image can be broken into segments by learning a mask for each segment. Image segmentation is used to locate object boundaries (lines, curves etc.) and assign a label to each pixel.\nFor example, this is from opencv documentation (watershed algorithm) where boundaries of a set of coins are learnt:\nOriginal Image Semantic Segmentation Mask Learnt from Original Image  Sudo Exam Tip: Remember to recall image segmentation as a technique when there is a mention of classification of objects in an image at pixel level. Each pixel is assigned a label when segmentation mask is learnt. The only thing is to be more specific when needed is to identify whether to use semantic or instance segmentation, which you\u0026rsquo;ll learn shortly.\n    Semantic and Instance Segmentation Compared  Semantic Segmentation Instance Segmentation     Objects in image are grouped based on defined categories Refined version of semantic segmentation   Example: A street scene would be segmented by pedestrians, bikes, vehicles, sidewalks etc. Example: Categories like \u0026ldquo;vehicles\u0026rdquo; are split into cars, buses, trucks etc.    Instance Segmentation detects each instance of a category      Sudo Exam Tip: Think of semantic segmentation as image classification at pixel level \u0026ndash; associates a pixel to a class label. Like image classification, semantic segmentation will not detect each distinct object in the image, rather a category of object.\n     Sudo Exam Tip: When given a choice among semantic and instance segmentation for autonomous vehicle use case -\u0026gt; choose instance segmentation.\n    ","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/semantic-vs-instance-segmentation/","summary":"Courtesy: Wikimedia\nWhat is segmentation of image in machine learning? In the field of computer vision in machine learning, an image can be broken into segments by learning a mask for each segment. Image segmentation is used to locate object boundaries (lines, curves etc.) and assign a label to each pixel.\nFor example, this is from opencv documentation (watershed algorithm) where boundaries of a set of coins are learnt:\nOriginal Image Semantic Segmentation Mask Learnt from Original Image  Sudo Exam Tip: Remember to recall image segmentation as a technique when there is a mention of classification of objects in an image at pixel level.","title":"What is the difference between Semantic and Instance Segmentation in Machine Learning?"},{"content":"Ready reference for MLS-C01 and SAP-C01: Amazon Kinesis Streams Compared  No need to study and find out inputs and outputs supported by Kinesis Streams by digging into AWS Documentation Here is a ready to refer comparision of Kinesis streams available on AWS Score points in exam:  Know the right sources and sinks that each stream supports Be cautious about tricky exam questions that state a wrong answer option involving kinesis streams    Kinesis Video Streams - KVS  Though the name is Kinesis Video Streams, it supports ingestion of any kind of time series or time encoded data Video:  Stream video frames to KVS for storage, analytics, machine learning, playback or custom video processing Kinesis Video Streams provides fully-managed capabilities for HTTP Live Streaming (HLS) and Dynamic Adaptive Streaming over HTTP (DASH). Kinesis Video Streams also supports ultra-low latency two-way media streaming with WebRTC, as a fully managed capability   Time Encoded Data:  Definition: Time-encoded data is any data in which the records are in a time series, and each record is related to its previous and next records e.g. video Non video examples, scientific applications:  LIDAR: LIght Detection And Ranging, is a remote sensing method that uses light in the form of a pulsed laser to measure ranges (variable distances) to the Earth RADAR: RAdio Detection And Ranging, is a detection system that uses radio waves to determine the range, angle, or velocity of objects. It can be used to detect aircraft, ships, spacecraft etc. Audio: any audio data     Write using:  PutMedia API Producer SDK   Read using:  GetMedia API Kinesis Video Streams Parser Library which can parse output from GetMedia API      Kinesis Video Streams I/O Integrations   Kinesis Data Streams - KDS  Write clickstreams, application logs, social media and thousands of sources to Amazon Kinesis data stream Write data to KDS using:  Kinesis Producer Library that manages concurrency and generates high throughput within limits of Kinesis Data Streams Use Kinesis API like PutRecords() from any compute e.g. Lambda, EC2 etc. and write to KDS Using kinesis agent   Read using:  Kinesis Client Library, from any app GetRecords() API Subscribe a Lambda where KDS is event source for a lambda      Kinesis Data Streams I/O Integrations   Kinesis Data Firehose - KDF  KDF is mainly meant only for data delivery unlike other streams It delivers realtime data from a source to various destinations with a transformation applied if need be Built-in support for JSON data to be written to Apache Parquet format to S3, no need of coding  Any other format, other than JSON, needs coding e.g. CSV These are called as lambda blueprints   Refer diagram below, to know data delivery destinations supported You can add data to an Amazon Kinesis Data Firehose delivery stream through Amazon Kinesis Agent or Firehose’s PutRecord and PutRecordBatch operations Kinesis Data Firehose is also integrated with other AWS data sources such as Kinesis Data Streams, AWS IoT, Amazon CloudWatch Logs, and Amazon CloudWatch Events Can write to firehose using kinesis agent  After installing Amazon Kinesis Agent on your servers, you can configure it to monitor certain files on the disk and then continuously send new data to your delivery stream. For more information, see Writing with Agents      Kinesis Data Firehose I/O Integrations   Kinesis Data Analytics - KDA  Realtime data analytics on streaming data using SQL Apply ML algorithms and perform anomaly detection  Anomaly detection is just an inbuilt SQL function that internally uses RCF - Random Cut Forest algorithm   Use Apache Flink applications in a supported language such as Java or Scala using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale Remember: KDA Cannot directly write to S3, attach a KDF i.e firehose to deliver KDA data to S3    Kinesis Data Analytics I/O Integrations   Tabular Comparison   Compare various streams at a glance   Leave your comments to appreciate the article or request for change. Thanks!\n","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/ai-ml-kinesis-compare/","summary":"Ready reference for MLS-C01 and SAP-C01: Amazon Kinesis Streams Compared  No need to study and find out inputs and outputs supported by Kinesis Streams by digging into AWS Documentation Here is a ready to refer comparision of Kinesis streams available on AWS Score points in exam:  Know the right sources and sinks that each stream supports Be cautious about tricky exam questions that state a wrong answer option involving kinesis streams    Kinesis Video Streams - KVS  Though the name is Kinesis Video Streams, it supports ingestion of any kind of time series or time encoded data Video:  Stream video frames to KVS for storage, analytics, machine learning, playback or custom video processing Kinesis Video Streams provides fully-managed capabilities for HTTP Live Streaming (HLS) and Dynamic Adaptive Streaming over HTTP (DASH).","title":"Amazon Kinesis Streams Compared"},{"content":"Ready reference for MLS-C01: Sagemaker Algorithms Compared  Study following table to compare various built-in algorithms in Sagemaker You can use this as a ready reckoner for MLS-C01 AWS Certified Machine Learning Specialty Exam Pay attention: Scroll the table horizontally for more columns Download the table as PDF here   Algorithm Algo. Type Input Format INT/FLOAT Processor Type Instance Multiprocessor\nin Single Machine Multi Machine Use Cases Comments HP     Linear Learner SUPERVISED • RecordIO Wrapped Protobuf / CSV\n• Float32 Data only\n FLOAT32 CPU\nGPU Any CPU\nGPU Only CPU\nNo GPU • Regression and classification\n• Classification: Binary or multiclass • Need data to be normalized else algo may not converge\n• Multiple models are trained in parallel balance_multiclass_weights\nlearning_rate\nmini_batch_Size\nL1, L2   XGBoost SUPERVISED • CSV or LibSVM (not AWS algo, but adapted, hence NO RecordIO-protobuf) - CPU M4 - No • Regression and classification\n• Classification: Binary or multiclass • Output model as pickle\n• Uses extreme boosting of trees\n• Algo is memory bound, not much compute • subsample_trees (less overfitting)\n• eta (eq. to learning rate)\n• alpha, gamma, lambda (conservative trees for higher values)   Seq2Seq SUPERVISED • RecordIO-Protobuf INT GPU P3 GPU No • Machine translation\n• Text summarization\n• Speech to text\n• Any use case where input a sequence and output is a sequence • Along with training data and validation data files, must provide vocabulary files \u0026ndash; in case of text seq2seq\n• Start with tokenized text files, then convert to RecordIO-Protobuf\n• Uses RNNs and CNNs internally • batch_size\n• optimizer\n• learning_rate\n• num_layers_encoder\n• num_layers_decoder\n• can optimize on: accuracy, BLEU score (mach. translation), perplexity\n   DeepAR SUPERVISED • JSON Lines\n• GZIP\n• Parquet\n-- Each record to contain\n- Start: starting TS\n- Target: the TS values to learn/predict - CPU\nGPU C4\nP3 CPU\nGPU CPU\nGPU • Stock price prediction\n• Sales and promotion effectiveness\n• Any time oriented forecasting, single dimension • Uses RNNs\n• Can train several related timeseries, more series the better results, learns relationships b/w timeseries\n• Start with CPU (C4.2xlarge, or higher), if necessary, move to GPU. Only large models need GPU • context_length (number of time points back in time the model learns)\n• epochs, batch_size, learning_rate, num_cells   Blazing Text - Text Classification SUPERVISED Augmented manifest text format \u0026ndash;\n\u0026quot;__label__1 this is a sentence with , punctuations also tokenized . that is space delimited . One sentence per line . label at the start\u0026quot; - CPU\nGPU size \u0026lt; 2GB: C5\nsize \u0026gt; 2GB: P2, P3 Single GPU No • web search and information retrieval • predict labels for sentence • epochs\n• learning_rate\n• word_ngrams\n• vector_dim   Blazing Text - Word2Vec UNSUPERVISED Word2Vec\none sentence per line\n - CPU\nGPU P3 CPU/GPU: CBOW \u0026amp; Skip Gram GPU: Batch skip gram\nCPU: No • Preparing input for NLP use cases\n• Vectorization of text for machine translation and sentiment analysis\n• Semantic similarity of words • Represents words as vectors\n• Semantically similar words are represented by vectors close to each other\n• Semantic \u0026ndash; of or relating to meaning in language\nMULTIPLE MODES:\n• CBOW - Continuous Bag of Words - Order of words DO NOT matter\n• Skip Gram i.e. n-gram - order of words matter\n• Batch skip gram - order of words matter\n • mode: mandatory\n• learning_rate\n• window_size\n• vector_dim\n• negative_Samples   Object2Vec  • Any object to be tokenized into integers\n• Training data:\n- pairs of tokens\n- sequence of tokens INT CPU\nGPU M5, P2 Single machine No • Collaborative recommendation system\n• Multi-label document classification system\n• Sentence Embeddings\n• Learns relations or associations:\n- sen to sen\n- labels to seq (genre to description)\n- product to product (recommendation)\n- user to item (recommendation) • CNNs and RNNs used\n• Encoders used in input\n- uses 2 encoders in parallel\n- learns associations b/w encoders, using a comparator\nEncoder types:\n• Hierchical CNNs (hCNNs)\n• bi-lstm\n• pooled_embedding\n dropout, early_stopping_ epochs, learning_rate, batch_size, layers, act. func., optimizer, weight_decay   Object Detection SUPERVISED RecordIO (NOT Protobuf) or Images (JPEG or PNG)\n+\nWith image manifest in JSON, one JSON per image\nthat contains annotations\n - GPU P2, P3 Yes Yes • Detect objects in an image\n• Object tracking • Uses CNN with SSD\n• Transfer learning/incremental learning supported\n• Uses FLIP, RESCALE, JITTER internally to avoid overfitting\n• CPUs can be used for inference, not for training Standard CNN HPs like: learning_rate, batch size, optimizer etc.   Image Classification SUPERVISED • Pipe: Apache MxNET RecordIO (NOT Protobuf) - for interoperability with other DNN frameworks\n• File Mode: Raw JPEG/PNG + *.LST files - associates image index, class label, path to image\n-- To use images directly in Pipe mode use JSON based Augmented Manifest Format - GPU P2, P3 Yes Yes • classify images into multiple classes\n• dog/cat/rat/tiger etc. • Full training: ResNet CNN is used. N/W initialized with random weights\n• Transfer Learning/Pre-trained: Image Net is used. Initialized with pre trained weights. Only Top FC layer is initialized with random weights.\n• CPU can be used for inference, if not suitable, move to GPU • batch_size\n• learning_rate\n• optimizer, B1, B2, eps, Gamma   Semantic Segmentation SUPERVISED • Raw JPEG/PNG in file mode + annotations\n• Add Augmented Manifest Format for Pipe Mode - GPU P2, P3 Yes No • Self driving cars\n• Medical imaging and diagnostics\n• Robot sensing\n• Given a pixel - what object does it belong to ? • Algo under hood: Gluon CV of MxNET = FC + Pyramid Scene Pairing + DeepLabV3\n• Arch: ResNet50/ResNet101 = \u0026ldquo;Backbone\u0026rdquo; selection in HP\n• Trained on ImageNet data\n• Incremental/Transfer learning allowed\n• Inference can use CPU or GPU\nEach of the three algorithms has two distinct components:\n• The backbone (or encoder)—A network that produces reliable activation maps of features.\n• The decoder—A network that constructs the segmentation mask from the encoded activation maps.\nThe segmentation output is represented as a grayscale image, called a segmentation mask. A segmentation mask is a grayscale image with the same shape as the input image. epochs, learning_rate, batch size, algo, backbone   Random Cut Forest UNSUPERVISED • RecordIO-Protobuf\n• CSV - CPU M4,C4,C5 - No • Anomaly detection\n• Detect unexpected spikes in TS data\n• Few people have tried using this for fraud detection • Assigns anomaly score to each data point\n• Uses forest of trees\n• Looks at expected change in complexity as a result of adding a point to a tree\n• Random sampling\n• RCF is used in Kinesis Analytics in real time num_trees, num_samples_per_tree (= choose inversely proportional to ratio #anomalous/#normal in dataset)\n   Neural Topic Modelling UNSUPERVISED • RecordIO-Protobuf\n• CSV\n- Words must be tokenized to integers\n- aux channel for vocab INT GPU P2, P3 -  • Organize docs into topics\n• Summarize docs based on topics • Algo: Neural Variational Inference\n• Define how many topics to group docs into\n• Used only on text\n• CPU / GPU for inference num_topics\nmini_batch_size\nlearning_rate\nvariation_loss (at expense of learning time)   LDA (Latent Dirichlet Allocation) UNSUPERVISED • RecordIO-Protobuf (Pipe Mode)\n• CSV\n- Words must be tokenized to integers\n- aux channel for vocab - CPU M4 No No • Cluster customers based on purchases\n• Harmonic analysis in music • Algo: LDA - Open source availability, not DNN\n• Can process more than text, like harmonic music analysis\n• Single inst. CPU\n num_topics\nalpha0 = small values - sparse topic mixtures, \u0026gt;1 uniform topic mixture   kNN (k Nearest Neighbors) SUPERVISED • RecordIO-protobuf\n• CSV\n-- File or pipe mode both\n- first column has label - CPU\nGPU - - - • Classification and regression • Sagemaker automates 3 steps:\n- Sample data (can\u0026rsquo;t use for huge data)\n- Dim reduction (sign or nfjlt methods)\n- Build index for looking up neighbours k\nsample_size   K-Means UNSUPERVISED • RecordIO-protobuf\n• CSV\n-- File or pipe mode both - CPU (recommended)\nGPU M4, M5, C4, C5 - - • Cluster data - unsupervised\n• Find groups of data points based on similarity • Webscale K-Means in Sagemaker\n• Similarity measured by euclidean distance\n• Works to optimize the centers of eack of the k-clusters\n• Algorithm:\n1) Determine init. cluster centers = 2 ways: k-means++ (tries to make initial clusters far apart) OR random\n2) Iterate over data and calculate cluster center\n3) Reduce from K to k - using Lloyd\u0026rsquo;s method or k-means++\nK comes from \u0026ldquo;extra_cluster_centers\u0026rdquo; which improves accuracy, but later reduced to k.\nK = k • x • K\n• mini_batch_size\n• extra_center_factor (x)\n• init_method (k-means++ OR random)   PCA - Principal Component Analysis UNSUPERVISED • RecordIO-protobuf\n• CSV\n-- File or pipe mode both - CPU\nGPU - - - • Dimensionality Reduction\n• Removes Curse of Dimensionality • Reduced Dimensions are called components\n• 1st component - largest possible variaility, next 2nd component, so on ..\n• Used Singular Value Decomposition (SVD)\n• Two Modes:\n- Regular: Sparse data. modelate #features, #rows\n- Randomized: Dense data. #large data, #large features, uses approximation algos • algorithm_mode (regular, random\n• subtract_mean: unbiases data   Factorization Machines SUPERVISED • RecordIO-Protobuf FLOAT32 CPU (recommended)\nGPU - - - • Regression, Classification, recommendation - all in one general purpose algo for sparse data\n• Click prediction\n• Item recommendation • Limited to pairwise interaction - 2nd order\ne.g. user to item interactions\n• CSV not practical hence not supported,a s data is sparse\n*GPU not recommented as data is sparse, GPU works better on dense data • Initialization methods for bias, factors and linear terms\n- methods: uniform, normal or const\n- can tune properties of each method   IP Insights UNSUPERVISED • CSV only for training\n• Inference: JSON lines, CSV, JSON - CPU\nGPU (recommended) - Multi GPU - • Identify suspicious IP addresses in context of security\n• Logins from anomalous IPs\n• Identify accounts creating resources from anamolous IPs • Only IPv4 supported\n• Uses NN to learn latent vector rep. of entities and IP addresses\n• entities are hashed and embedded - large hash size\n• Automatically generates anomalous data by randomly pairing entities and IPs - as data will be highly imbalanced • num_entity_vectors (hash size, set to twice the unique entity identifiers)\n• vector_dim (size of embedding vectors, scales model size)\n• Others: epoch, batch_size, leraning_rate, etc.   Reinforcement Learning REINFORCEMENT LEARNING • Nothing specific to Sagemaker - GPU GPU Yes Yes - Multi Instance GPU recommended • Games\n• Supply chain management\n• HVAC Systems\n• Industrial robotics\n• Dialog systems\n• Autonomous vehicles • Supports Intel coach, Ray RLLib\n• Tensorflow, MxNET\n• Custom, commercial and opensource environments supported - Matlab simulink, energy plus, robo school, pybullet, Amazon Sumerian, AWS Robomaker • Depends on framework and algo used, nothing tied to Sagemaker     Algorithms categorized by feature  Here a feature is based on:  Data type of input Algorithms that can train on CPU or GPU Algorithms that can be trained incrementally  Incremental training enables to resume training, or to retrain a DNN by changing the final FC layer by using a pre-trained model i.e transfer learning       Mandatory FLOAT32 Mandatory INT32 CPU ONLY GPU Only Incremental Training Available     Linear Learner Seq2Seq XGBoost Seq2Seq Image Classification   Factorization Machines Object2Vec RCF Image Classification Semantic Segmentation    NTM LDA Semantic Segmentation Object Detection      Object Detection       NTM       Reinforcement Learning      Algorithms that support distributed training  Mnemonic to remember: F-SKILBDR as in SkillBuilder (If you can come up with something better, please share in comments! ) An entire new blog to understand distributed training and how it works coming soon!   Distributed Training Support     Factorization Machine   Seq2Seq   K-Means   IP Insights   Linear Learner (Not LDA)   Blazing Text - Word2Vec   DeepAR   RCF - Random Cut Forests     Leave your comments to appreciate the article or request for change. Thanks!\n","permalink":"https://sudo-code7.github.io/posts/tech/ml/mls-c01/ai-ml-mls-c01/","summary":"Ready reference for MLS-C01: Sagemaker Algorithms Compared  Study following table to compare various built-in algorithms in Sagemaker You can use this as a ready reckoner for MLS-C01 AWS Certified Machine Learning Specialty Exam Pay attention: Scroll the table horizontally for more columns Download the table as PDF here   Algorithm Algo. Type Input Format INT/FLOAT Processor Type Instance Multiprocessor\nin Single Machine Multi Machine Use Cases Comments HP     Linear Learner SUPERVISED • RecordIO Wrapped Protobuf / CSV","title":"Amazon Sagemaker Built-in Algorithms Compared"}]